# Circuit: projection-identity_sub_glue-Softmax
!obj:pylearn2.train.Train {
    dataset: !!python/object/apply:grafl.dataset.edge_dataset.BWD_dataset ['train'] ,
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [
            !obj:pylearn2.sandbox.nlp.models.mlp.ProjectionLayer {
                layer_name: 'projection',
                dim: &emb_dim 80, # Even 5 dimensional embedding can get 94% acc!
                irange: &irange 0.01, # NOTE: This could be set to 0.05
            },
            !obj:glue_composite_layer.GlueLayer {
                layer_name: 'substractive_glue',
                dim: *emb_dim ,
                nonlinearity: &activation !!python/name:pylearn2.expr.activations.identity ,
                operation: &operator !!python/name:operator.sub ,
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: &chromaticity 3,
                irange: *irange ,
            },
        ],
        input_space: !!python/name:grafl.dataset.edge_dataset.BWD_input_space ,
        input_source: !!python/name:grafl.dataset.edge_dataset.BWD_input_source ,
        target_source: !!python/name:grafl.dataset.edge_dataset.BWD_target_source ,
    },
    algorithm: !include:res/experiments/taml/learning_algorithm.taml {
        weight_decay_tactic: &weight_decay_tactic 'weight_decay_with_default.WeightDecayWithDefault { default: 5e-5 }',
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
            channel_name: 'valid_y_misclass',
            save_path: '${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl',
        },
    ],
    save_path: '${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl',
    save_freq: 1,
}
# Local Variables:
# eval: (read-only-mode)
# End:
# -*- OptionsBegin -*-
# emb_dim: 80 50 25 5
# irange: 0.01 0.05
# activation: !!python/name:pylearn2.expr.activations.identity !!python/name:pylearn2.expr.activations.relu
# operator: !!python/name:operator.sub !!python/name:operator.add !!python/name:operator.mul
# weight_decay_tactic: 'weight_decay_with_default.WeightDecayWithDefault { default: 0.0 }' 'pylearn2.costs.mlp.WeightDecay {coeffs: {optional_layer: 5e-5, comparator: 5e-5}}'
